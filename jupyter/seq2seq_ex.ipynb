{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 코퍼스 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Help extends checkout.</td>\n",
       "      <td>\\t (help) .&gt; (checkout) : extends \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The checkouts require payment.</td>\n",
       "      <td>\\t (checkout) .&gt; (payment) : include \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>The customer does checkout.</td>\n",
       "      <td>\\t customer -- (checkout) \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Help extends checkout.</td>\n",
       "      <td>\\t (help) .&gt; (checkout) : extends \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 src                                      tar\n",
       "530           Help extends checkout.     \\t (help) .> (checkout) : extends \\n\n",
       "29    The checkouts require payment.  \\t (checkout) .> (payment) : include \\n\n",
       "1384     The customer does checkout.             \\t customer -- (checkout) \\n\n",
       "1082          Help extends checkout.     \\t (help) .> (checkout) : extends \\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# lines= pd.read_csv('fra.txt', names=['src', 'tar', ''], sep='\\t')  # 학습할 데이터 위치\n",
    "# len(lines)\n",
    "\n",
    "# # 전체 샘플의 개수\n",
    "# lines = lines[0:5000] # 6만개만 저장\n",
    "# lines.sample(10)\n",
    "\n",
    "# # <sos>와 <eos> 대신 \\t를 시작 심볼, \\n을 종료 심볼로 간주하여 다시 출력\n",
    "# lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "# lines.sample(10)\n",
    "\n",
    "TRAIN_CSV_PATH = 'train.csv' # Train 데이터 위치\n",
    "TEST_CSV_PATH = 'test.csv' # Test 데이터 위치\n",
    "\n",
    "lines= pd.read_csv(TRAIN_CSV_PATH, names=['src', 'tar'], sep=',', skiprows=[0])  # 학습할 데이터 위치\n",
    "len(lines)\n",
    "\n",
    "# 전체 샘플의 개수\n",
    "lines = lines[0:5000] # 6만개만 저장\n",
    "lines.sample(4)\n",
    "\n",
    "# <sos>와 <eos> 대신 \\t를 시작 심볼, \\n을 종료 심볼로 간주하여 다시 출력\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', 'q', 'r', 'l', 'k', 'x', 'C', 'm', 'p', 'T', 'e', 'o', 'a', '.', 's', 'h', 'u', 't', 'H', 'c', 'b', 'd', ' ', 'y', 'n'}\n",
      "{':', 'i', 'r', 'l', 'k', 'x', 'm', 'p', 'o', 'e', '\\t', 'a', '.', 's', 'h', 'u', 't', 'c', '\\n', '>', 'n', ')', 'd', '(', ' ', 'y', '-'}\n"
     ]
    }
   ],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab=set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# 글자 집합의 크기 확인\n",
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 1, 'l': 2, 'r': 3, 'C': 4, 'm': 5, 'e': 6, 'a': 7, 'p': 8, 'i': 9, 'x': 10, 'h': 11, 't': 12, 'n': 13, 'k': 14, ' ': 15, 'H': 16, 'o': 17, '.': 18, 's': 19, 'b': 20, 'y': 21, 'u': 22, 'q': 23, 'T': 24, 'c': 25}\n",
      "{'d': 1, 'l': 2, 'r': 3, '-': 4, '\\n': 5, 'm': 6, ':': 7, '>': 8, 'e': 9, 'a': 10, 'p': 11, 'i': 12, 'x': 13, 'h': 14, '\\t': 15, '(': 16, ')': 17, 't': 18, 'n': 19, 'k': 20, ' ': 21, 'o': 22, '.': 23, 's': 24, 'y': 25, 'u': 26, 'c': 27}\n"
     ]
    }
   ],
   "source": [
    "# 각 글자에 인덱스 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 11, 6, 15, 25, 22, 19, 12, 17, 5, 6, 3, 15, 1, 17, 6, 19, 15, 25, 11, 6, 25, 14, 17, 22, 12, 18], [24, 11, 6, 15, 25, 11, 6, 25, 14, 17, 22, 12, 19, 15, 3, 6, 23, 22, 9, 3, 6, 15, 8, 7, 21, 5, 6, 13, 12, 18], [16, 6, 2, 8, 15, 6, 10, 12, 6, 13, 1, 19, 15, 25, 11, 6, 25, 14, 17, 22, 12, 18], [4, 11, 6, 25, 14, 17, 22, 12, 15, 9, 19, 15, 1, 17, 13, 6, 15, 20, 21, 15, 25, 2, 6, 3, 14, 18], [24, 11, 6, 15, 25, 22, 19, 12, 17, 5, 6, 3, 15, 1, 17, 6, 19, 15, 25, 11, 6, 25, 14, 17, 22, 12, 18]]\n"
     ]
    }
   ],
   "source": [
    "# 인코더의 입력이 될 한글 문장 샘플에 대해서 정수 인코딩 수행\n",
    "# 5개의 샘플만 출력\n",
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 21, 27, 26, 24, 18, 22, 6, 9, 3, 21, 4, 4, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 5], [15, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 23, 8, 21, 16, 11, 10, 25, 6, 9, 19, 18, 17, 21, 7, 21, 12, 19, 27, 2, 26, 1, 9, 21, 5], [15, 21, 16, 14, 9, 2, 11, 17, 21, 23, 8, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 7, 21, 9, 13, 18, 9, 19, 1, 24, 21, 5], [15, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 4, 4, 21, 27, 2, 9, 3, 20, 21, 5], [15, 21, 27, 26, 24, 18, 22, 6, 9, 3, 21, 4, 4, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 5]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 입력이 될 영어 데이터에 대해서 정수 인코딩 수행\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21, 27, 26, 24, 18, 22, 6, 9, 3, 21, 4, 4, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 5], [21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 23, 8, 21, 16, 11, 10, 25, 6, 9, 19, 18, 17, 21, 7, 21, 12, 19, 27, 2, 26, 1, 9, 21, 5], [21, 16, 14, 9, 2, 11, 17, 21, 23, 8, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 7, 21, 9, 13, 18, 9, 19, 1, 24, 21, 5], [21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 4, 4, 21, 27, 2, 9, 3, 20, 21, 5], [21, 27, 26, 24, 18, 22, 6, 9, 3, 21, 4, 4, 21, 16, 27, 14, 9, 27, 20, 22, 26, 18, 17, 21, 5]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 예측값과 비교하기 위한 실제값 (영어 문장)의 맨 앞에 붙어있는 \\t 제거\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# 패딩을 위해서 한국어 문장과 영어 문장 각각에 대해서,\n",
    "# 가장 길이가 긴 샘플의 길이를 알아보겠습니다.\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 가장 긴 샘플의 길이에 맞춰서,\n",
    "# 한국어 데이터의 샘플은 전부 길이가 25가 되도록 패딩하고,\n",
    "# 영어 데이터의 샘플은 전부 길이가 76이 되도록 패딩합니다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 모든 값에 대해서 원-핫 인코딩 수행\n",
    "# 실제값, 입력값 모두 원-핫 벡터 사용\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "\n",
    "# 데이터 전처리 끝!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 기계 번역기 훈련시키기 (교사 강요 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152 samples, validate on 288 samples\n",
      "Epoch 1/50\n",
      "1152/1152 [==============================] - 6s 5ms/sample - loss: 2.7114 - val_loss: 2.2122\n",
      "Epoch 2/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 2.1426 - val_loss: 1.9201\n",
      "Epoch 3/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.8922 - val_loss: 1.6717\n",
      "Epoch 4/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.6529 - val_loss: 1.5456\n",
      "Epoch 5/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.3799 - val_loss: 1.3672\n",
      "Epoch 6/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.1875 - val_loss: 1.2647\n",
      "Epoch 7/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.9950 - val_loss: 0.7918\n",
      "Epoch 8/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.7726 - val_loss: 0.6889\n",
      "Epoch 9/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.6743 - val_loss: 0.4886\n",
      "Epoch 10/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.5186 - val_loss: 0.4273\n",
      "Epoch 11/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.3839 - val_loss: 0.2579\n",
      "Epoch 12/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.3596 - val_loss: 0.2342\n",
      "Epoch 13/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.2049 - val_loss: 0.1607\n",
      "Epoch 14/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.1922 - val_loss: 0.1025\n",
      "Epoch 15/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.1549 - val_loss: 0.0789\n",
      "Epoch 16/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0802 - val_loss: 0.3736\n",
      "Epoch 17/50\n",
      "1152/1152 [==============================] - 3s 3ms/sample - loss: 0.0949 - val_loss: 0.0473\n",
      "Epoch 18/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0548 - val_loss: 0.2087\n",
      "Epoch 19/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.2007 - val_loss: 0.0393\n",
      "Epoch 20/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0364 - val_loss: 0.0294\n",
      "Epoch 21/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0447 - val_loss: 0.0248\n",
      "Epoch 22/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0269 - val_loss: 0.0210\n",
      "Epoch 23/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0275 - val_loss: 0.0953\n",
      "Epoch 24/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0386 - val_loss: 0.0122\n",
      "Epoch 25/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0208 - val_loss: 0.0075\n",
      "Epoch 26/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0094 - val_loss: 0.0033\n",
      "Epoch 27/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 28/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0420 - val_loss: 0.0143\n",
      "Epoch 29/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0072 - val_loss: 0.0025\n",
      "Epoch 30/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 31/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 9.3897e-04 - val_loss: 7.0119e-04\n",
      "Epoch 32/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 5.5808e-04 - val_loss: 4.0886e-04\n",
      "Epoch 33/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0537 - val_loss: 0.0035\n",
      "Epoch 34/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 35/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0011 - val_loss: 7.7392e-04\n",
      "Epoch 36/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 5.8383e-04 - val_loss: 4.1992e-04\n",
      "Epoch 37/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 3.3612e-04 - val_loss: 2.5800e-04\n",
      "Epoch 38/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 2.0333e-04 - val_loss: 1.4822e-04\n",
      "Epoch 39/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0456 - val_loss: 0.0253\n",
      "Epoch 40/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 0.0022 - val_loss: 6.8835e-04\n",
      "Epoch 41/50\n",
      "1152/1152 [==============================] - 3s 3ms/sample - loss: 5.2822e-04 - val_loss: 3.9103e-04\n",
      "Epoch 42/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 3.1151e-04 - val_loss: 2.3743e-04\n",
      "Epoch 43/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.8991e-04 - val_loss: 1.4436e-04\n",
      "Epoch 44/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.1481e-04 - val_loss: 8.6298e-05\n",
      "Epoch 45/50\n",
      "1152/1152 [==============================] - 3s 3ms/sample - loss: 6.7402e-05 - val_loss: 4.9121e-05\n",
      "Epoch 46/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 3.7081e-05 - val_loss: 2.6048e-05\n",
      "Epoch 47/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.9830e-05 - val_loss: 1.4083e-05\n",
      "Epoch 48/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 1.0693e-05 - val_loss: 7.6182e-06\n",
      "Epoch 49/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 5.8607e-06 - val_loss: 4.2783e-06\n",
      "Epoch 50/50\n",
      "1152/1152 [==============================] - 3s 2ms/sample - loss: 3.3894e-06 - val_loss: 2.6029e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21bfa424fc8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 에포크 학습\n",
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 정의\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는\n",
    "# index_to_src와 index_to_tar 생성\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
    "\n",
    "# print(\"index_to_src: \")\n",
    "# for k, v in index_to_src.items():\n",
    "#     print(k, v)\n",
    "    \n",
    "# print(\"index_to_tar: \")\n",
    "# for k, v in index_to_tar.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "#             len(decoded_sentence) > max_tar_len):\n",
    "            len(decoded_sentence) > max_tar_len - 10):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Checkout is done by clerk.\n",
      "정답 문장:  (checkout) -- clerk \n",
      "번역기가 번역한 문장:  (checkout) -- clerk \n",
      "-----------------------------------\n",
      "입력 문장: Help extends checkout.\n",
      "정답 문장:  (help) .> (checkout) : extends \n",
      "번역기가 번역한 문장:  (help) .> (checkout) : ext\n",
      "-----------------------------------\n",
      "입력 문장: The customer does checkout.\n",
      "정답 문장:  customer -- (checkout) \n",
      "번역기가 번역한 문장:  customer -- (checkout) \n",
      "-----------------------------------\n",
      "입력 문장: The customer does checkout.\n",
      "정답 문장:  customer -- (checkout) \n",
      "번역기가 번역한 문장:  customer -- (checkout) \n",
      "-----------------------------------\n",
      "입력 문장: The checkouts require payment.\n",
      "정답 문장:  (checkout) .> (payment) : include \n",
      "번역기가 번역한 문장:  (checkout) .> (payment) : \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3, 50, 100, 300, 1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
